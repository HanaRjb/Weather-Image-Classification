{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S-y-wCf_MFGS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "263d690e-3655-4e30-a5e9-38a3f8afa0bb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: split-folders in /usr/local/lib/python3.10/dist-packages (0.5.1)\n"
          ]
        }
      ],
      "source": [
        "!pip install split-folders"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7ThtC4EHs1Rq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "48be31b0-fa30-4334-88a7-11da776b5826"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive/\n"
          ]
        }
      ],
      "source": [
        "import zipfile\n",
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/drive/')\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GqTZtitMs4zN"
      },
      "outputs": [],
      "source": [
        "zip_ref = zipfile.ZipFile(\"/content/drive/MyDrive/archive(1).zip\", 'r')\n",
        "zip_ref.extractall(\"/content\")\n",
        "zip_ref.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fsUsXlntt4yH"
      },
      "outputs": [],
      "source": [
        "import splitfolders\n",
        "from keras.applications.vgg19 import VGG19\n",
        "from keras import models\n",
        "from keras import layers\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# A. Split the weather dataset into three train, validation, and test folders with a ratio of 0.8, 0.1, and 0.1 using the splitfolders library.\n",
        "\n",
        "input_folder = '/content/dataset'\n",
        "output_folder = '/content/output_folder'\n",
        "\n",
        "splitfolders.ratio(input_folder, output_folder, seed=42, ratio=(0.8, 0.1, 0.1))\n",
        "\n",
        "# B. Use the VGG19 network which trained on the ImageNet dataset. The VGG19 has the following architecture:\n",
        "\n",
        "base_model = VGG19(weights='imagenet', include_top=False, input_shape=(150, 150, 3))\n",
        "\n",
        "# C. Resize the images to 150 x 150.\n",
        "\n",
        "train_datagen = ImageDataGenerator(rescale=1./255,\n",
        "                                   width_shift_range=0.2,\n",
        "                                   height_shift_range=0.2,\n",
        "                                   shear_range=0.2,\n",
        "                                   zoom_range=0.2,\n",
        "                                   horizontal_flip=True)\n",
        "\n",
        "test_datagen = ImageDataGenerator(rescale=1./255)\n",
        "\n",
        "train_dir = output_folder + '/train'\n",
        "val_dir = output_folder + '/val'\n",
        "test_dir = output_folder + '/test'\n",
        "\n",
        "train_generator = train_datagen.flow_from_directory(train_dir,\n",
        "                                                    target_size=(150, 150),\n",
        "                                                    batch_size=32,\n",
        "                                                    class_mode='categorical')\n",
        "\n",
        "validation_generator = test_datagen.flow_from_directory(val_dir,\n",
        "                                                        target_size=(150, 150),\n",
        "                                                        batch_size=32,\n",
        "                                                        class_mode='categorical')\n",
        "\n",
        "\n",
        "# D. Remove the network's top layers containing the classification layers.\n",
        "\n",
        "model = models.Sequential()\n",
        "model.add(base_model)\n",
        "model.add(layers.Flatten())\n",
        "model.add(layers.Dense(256, activation='relu'))\n",
        "model.add(layers.Dropout(0.5))\n",
        "model.add(layers.Dense(11, activation='softmax'))\n",
        "\n",
        "base_model.trainable = False\n",
        "\n",
        "model.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# E. Use the VGG19 model for feature extraction without data augmentation.\n",
        "\n",
        "history = model.fit(train_generator,\n",
        "                    epochs=50,\n",
        "                    batch_size=32,\n",
        "                    validation_data=validation_generator)\n",
        "\n",
        "# F. Add dense layers for the classification part.\n",
        "\n",
        "base_model.trainable = True\n",
        "\n",
        "set_trainable = False\n",
        "for layer in base_model.layers:\n",
        "    if layer.name == 'block5_conv1':\n",
        "        set_trainable = True\n",
        "    if set_trainable:\n",
        "        layer.trainable = True\n",
        "    else:\n",
        "        layer.trainable = False\n",
        "\n",
        "model.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "history_fine_tuned = model.fit(train_generator,\n",
        "                               epochs=50,\n",
        "                               batch_size=32,\n",
        "                               validation_data=validation_generator)\n",
        "\n",
        "# G. In this part, extract features using data augmentation, which means extending the VGG19 model and running it end to end on the inputs.\n",
        "\n",
        "train_datagen_aug = ImageDataGenerator(rescale=1./255,\n",
        "                                       width_shift_range=0.2,\n",
        "                                       height_shift_range=0.2,\n",
        "                                       shear_range=0.2,\n",
        "                                       zoom_range=0.2,\n",
        "                                       horizontal_flip=True)\n",
        "\n",
        "test_datagen_aug = ImageDataGenerator(rescale=1./255)\n",
        "\n",
        "train_generator_aug = train_datagen_aug.flow_from_directory(train_dir,\n",
        "                                                            target_size=(150, 150),\n",
        "                                                            batch_size=32,\n",
        "                                                            class_mode='categorical')\n",
        "\n",
        "validation_generator_aug = test_datagen_aug.flow_from_directory(val_dir,\n",
        "                                                                target_size=(150, 150),\n",
        "                                                                batch_size=32,\n",
        "                                                                class_mode='categorical')\n",
        "\n",
        "model.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "history_aug = model.fit(train_generator_aug,\n",
        "                        epochs=50,\n",
        "                        batch_size=32,\n",
        "                        validation_data=validation_generator_aug)\n",
        "\n",
        "# Plotting the accuracy and loss for feature extraction without data augmentation\n",
        "\n",
        "acc = history.history['accuracy']\n",
        "val_acc = history.history['val_accuracy']\n",
        "loss = history.history['loss']\n",
        "val_loss = history.history['val_loss']\n",
        "\n",
        "epochs = range(1, len(acc) + 1)\n",
        "\n",
        "plt.plot(epochs, acc, 'bo', label='Training acc')\n",
        "plt.plot(epochs, val_acc, 'b', label='Validation acc')\n",
        "plt.title('Training and validation accuracy without data augmentation')\n",
        "plt.legend()\n",
        "\n",
        "plt.figure()\n",
        "\n",
        "plt.plot(epochs, loss, 'bo', label='Training loss')\n",
        "plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
        "plt.title('Training and validation loss without data augmentation')\n",
        "plt.legend()\n",
        "\n",
        "plt.figure()\n",
        "\n",
        "# Plotting the accuracy and loss for fine-tuned VGG19 model\n",
        "\n",
        "acc = history_fine_tuned.history['accuracy']\n",
        "val_acc = history_fine_tuned.history['val_accuracy']\n",
        "loss = history_fine_tuned.history['loss']\n",
        "val_loss = history_fine_tuned.history['val_loss']\n",
        "\n",
        "epochs = range(1, len(acc) + 1)\n",
        "\n",
        "plt.plot(epochs, acc, 'bo', label='Training acc')\n",
        "plt.plot(epochs, val_acc, 'b', label='Validation acc')\n",
        "plt.title('Training and validation accuracy with fine-tuned VGG19 model')\n",
        "plt.legend()\n",
        "\n",
        "plt.figure()\n",
        "\n",
        "plt.plot(epochs, loss, 'bo', label='Training loss')\n",
        "plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
        "plt.title('Training and validation loss with fine-tuned VGG19 model')\n",
        "plt.legend()\n",
        "\n",
        "plt.figure()\n",
        "\n",
        "# Plotting the accuracy and loss for feature extraction with data augmentation\n",
        "\n",
        "acc = history_aug.history['accuracy']\n",
        "val_acc = history_aug.history['val_accuracy']\n",
        "loss = history_aug.history['loss']\n",
        "val_loss = history_aug.history['val_loss']\n",
        "\n",
        "epochs = range(1, len(acc) + 1)\n",
        "\n",
        "plt.plot(epochs, acc, 'bo', label='Training acc')\n",
        "plt.plot(epochs, val_acc, 'b', label='Validation acc')\n",
        "plt.title('Training and validation accuracy with feature extraction and data augmentation')\n",
        "plt.legend()\n",
        "\n",
        "plt.figure()\n",
        "\n",
        "plt.plot(epochs, loss, 'bo', label='Training loss')\n",
        "plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
        "plt.title('Training and validation loss with feature extraction and data augmentation')\n",
        "plt.legend()\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l-fDNCaGvksa"
      },
      "outputs": [],
      "source": [
        "import splitfolders\n",
        "from keras.applications.vgg19 import VGG19\n",
        "from keras import models\n",
        "from keras import layers\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# A. Split the weather dataset into three train, validation, and test folders with a ratio of 0.8, 0.1, and 0.1 using the splitfolders library.\n",
        "input_folder = '/content/dataset'\n",
        "output_folder = '/content/output_folder'\n",
        "\n",
        "splitfolders.ratio(input_folder, output_folder, seed=42, ratio=(0.8, 0.1, 0.1))\n",
        "\n",
        "# B. Use the VGG19 network which trained on the ImageNet dataset.\n",
        "base_model = VGG19(weights='imagenet', include_top=False, input_shape=(150, 150, 3))\n",
        "\n",
        "# C. Resize the images to 150 x 150 and apply data augmentation to the training set.\n",
        "train_datagen = ImageDataGenerator(rescale=1./255,\n",
        "                                   width_shift_range=0.2,\n",
        "                                   height_shift_range=0.2,\n",
        "                                   shear_range=0.2,\n",
        "                                   zoom_range=0.2,\n",
        "                                   horizontal_flip=True)\n",
        "\n",
        "test_datagen = ImageDataGenerator(rescale=1./255)\n",
        "\n",
        "train_dir = output_folder + '/train'\n",
        "val_dir = output_folder + '/val'\n",
        "test_dir = output_folder + '/test'\n",
        "\n",
        "train_generator = train_datagen.flow_from_directory(train_dir,\n",
        "                                                    target_size=(150, 150),\n",
        "                                                    batch_size=32,\n",
        "                                                    class_mode='categorical')\n",
        "\n",
        "validation_generator = test_datagen.flow_from_directory(val_dir,\n",
        "                                                        target_size=(150, 150),\n",
        "                                                        batch_size=32,\n",
        "                                                        class_mode='categorical')\n",
        "\n",
        "# D. Remove the network's top layers containing the classification layers and add dense layers for the classification part, including batch normalization.\n",
        "model = models.Sequential()\n",
        "model.add(base_model)\n",
        "model.add(layers.Flatten())\n",
        "model.add(layers.Dense(256))\n",
        "model.add(layers.BatchNormalization())\n",
        "model.add(layers.Activation('relu'))\n",
        "model.add(layers.Dropout(0.5))\n",
        "model.add(layers.Dense(11, activation='softmax'))\n",
        "\n",
        "base_model.trainable = False\n",
        "\n",
        "model.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# E. Fine-tune the model by making the last convolutional block trainable.\n",
        "for layer in base_model.layers:\n",
        "    if layer.name.startswith('block5'):\n",
        "        layer.trainable = True\n",
        "    else:\n",
        "        layer.trainable = False\n",
        "\n",
        "model.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "history_fine_tuned = model.fit(train_generator,\n",
        "                               epochs=50,\n",
        "                               batch_size=32,\n",
        "                               validation_data=validation_generator)\n",
        "\n",
        "# F. Plot the accuracy and loss for training and validation data.\n",
        "acc = history_fine_tuned.history['accuracy']\n",
        "val_acc = history_fine_tuned.history['val_accuracy']\n",
        "loss = history_fine_tuned.history['loss']\n",
        "val_loss = history_fine_tuned.history['val_loss']\n",
        "\n",
        "epochs = range(1, len(acc) + 1)\n",
        "\n",
        "plt.plot(epochs, acc, 'bo', label='Training acc')\n",
        "plt.plot(epochs, val_acc, 'b', label='Validation acc')\n",
        "plt.title('Training and validation accuracy (fine-tuned last block)')\n",
        "plt.legend()\n",
        "\n",
        "plt.figure()\n",
        "\n",
        "plt.plot(epochs, loss, 'bo', label='Training loss')\n",
        "plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
        "plt.title('Training and validation loss (fine-tuned last block)')\n",
        "plt.legend()\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YJKj1ye_eKCV"
      },
      "outputs": [],
      "source": [
        "import splitfolders\n",
        "from keras.applications.vgg19 import VGG19\n",
        "from keras import models\n",
        "from keras import layers\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# A. Split the weather dataset into three train, validation, and test folders with a ratio of 0.8, 0.1, and 0.1 using the splitfolders library.\n",
        "input_folder = '/content/dataset'\n",
        "output_folder = '/content/output_folder'\n",
        "\n",
        "splitfolders.ratio(input_folder, output_folder, seed=42, ratio=(0.8, 0.1, 0.1))\n",
        "\n",
        "# B. Use the VGG19 network which trained on the ImageNet dataset.\n",
        "base_model = VGG19(weights='imagenet', include_top=False, input_shape=(150, 150, 3))\n",
        "\n",
        "# C. Resize the images to 150 x 150 and apply data augmentation to the training set.\n",
        "train_datagen = ImageDataGenerator(rescale=1./255,\n",
        "                                   width_shift_range=0.2,\n",
        "                                   height_shift_range=0.2,\n",
        "                                   shear_range=0.2,\n",
        "                                  zoom_range=0.2,\n",
        "                                   horizontal_flip=True)\n",
        "\n",
        "test_datagen = ImageDataGenerator(rescale=1./255)\n",
        "\n",
        "train_dir = output_folder + '/train'\n",
        "val_dir = output_folder + '/val'\n",
        "test_dir = output_folder + '/test'\n",
        "\n",
        "train_generator = train_datagen.flow_from_directory(train_dir,\n",
        "                                                    target_size=(150, 150),\n",
        "                                                    batch_size=32,\n",
        "                                                    class_mode='categorical')\n",
        "\n",
        "validation_generator = test_datagen.flow_from_directory(val_dir,\n",
        "                                                        target_size=(150, 150),\n",
        "                                                        batch_size=32,\n",
        "                                                        class_mode='categorical')\n",
        "\n",
        "# D. Remove the network's top layers containing the classification layers and add dense layers for the classification part, including batch normalization.\n",
        "model = models.Sequential()\n",
        "model.add(base_model)\n",
        "model.add(layers.Flatten())\n",
        "model.add(layers.Dense(256))\n",
        "model.add(layers.BatchNormalization())\n",
        "model.add(layers.Activation('relu'))\n",
        "model.add(layers.Dropout(0.5))\n",
        "model.add(layers.Dense(11, activation='softmax'))\n",
        "\n",
        "base_model.trainable = False\n",
        "\n",
        "model.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# E. Fine-tune the model by making the last two convolutional blocks trainable.\n",
        "for layer in base_model.layers:\n",
        "    if layer.name.startswith('block4') or layer.name.startswith('block5'):\n",
        "        layer.trainable = True\n",
        "    else:\n",
        "        layer.trainable = False\n",
        "\n",
        "model.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "history_fine_tuned = model.fit(train_generator,\n",
        "                               epochs=50,\n",
        "                               batch_size=32,\n",
        "                               validation_data=validation_generator)\n",
        "\n",
        "# F. Plot the accuracy and loss for training and validation data.\n",
        "acc = history_fine_tuned.history['accuracy']\n",
        "val_acc = history_fine_tuned.history['val_accuracy']\n",
        "loss = history_fine_tuned.history['loss']\n",
        "val_loss = history_fine_tuned.history['val_loss']\n",
        "\n",
        "epochs = range(1, len(acc) + 1)\n",
        "\n",
        "plt.plot(epochs, acc, 'bo', label='Training acc')\n",
        "plt.plot(epochs, val_acc, 'b', label='Validation acc')\n",
        "plt.title('Training and validation accuracy (fine-tuned last two blocks)')\n",
        "plt.legend()\n",
        "\n",
        "plt.figure()\n",
        "\n",
        "plt.plot(epochs, loss, 'bo', label='Training loss')\n",
        "plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
        "plt.title('Training and validation loss (fine-tuned last two blocks)')\n",
        "plt.legend()\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "tSI5BtdX1ufS"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}